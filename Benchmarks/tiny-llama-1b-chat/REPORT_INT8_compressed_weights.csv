iteration,model,framework,device,pretrain_time(s),input_size,infer_count,generation_time(s),output_size,latency(ms),1st_latency(ms),2nd_avg_latency(ms),precision,max_rss_mem(MB),max_shared_mem(MB),prompt_idx,1st_infer_latency(ms),2nd_infer_avg_latency(ms),num_beams,batch_size,tokenization_time,detokenization_time,result_md5
0,tiny-llama-1b-chat,ov(INT8_compressed_weights),cpu,8.48926,8,512,5.31835,11,483.48664,4749.1847,52.66283,INT8_compressed_weights,,,0,4699.826,51.91093,1,1,6.027,5.7974,['2880615b55b875d480212888b4d5a624']
1,tiny-llama-1b-chat,ov(INT8_compressed_weights),cpu,,8,512,0.69315,11,63.01341,134.6699,55.78145,INT8_compressed_weights,,,0,134.0588,55.04197,1,1,0.1607,0.1796,['2880615b55b875d480212888b4d5a624']
2,tiny-llama-1b-chat,ov(INT8_compressed_weights),cpu,,8,512,0.7968,11,72.43641,160.3558,63.5249,INT8_compressed_weights,,,0,159.7249,62.65228,1,1,0.2805,0.1963,['2880615b55b875d480212888b4d5a624']
-1,tiny-llama-1b-chat,ov(INT8_compressed_weights),cpu,,8.0,512.0,0.74497,11.0,67.72491,147.51285,59.65317,INT8_compressed_weights,,,0,146.89185,58.84712,1,1,0.2805,0.1963,['2880615b55b875d480212888b4d5a624']
,,,,,,,,,,,,,,,,,,,,,,
input_size: Input token size,,,,,,,,,,,,,,,,,,,,,,
output_size: Text/Code generation models: generated text token size,,,,,,,,,,,,,,,,,,,,,,
infer_count: Limit the Text/Code generation models' output token size,,,,,,,,,,,,,,,,,,,,,,
latency: Text/Code generation models: ms/token. Output token size / generation time,,,,,,,,,,,,,,,,,,,,,,
1st_latency: Text/Code generation models: Fisrt token latency,,,,,,,,,,,,,,,,,,,,,,
2nd_avg_latency: Text/Code generation models: Other tokens (exclude first token) latency,,,,,,,,,,,,,,,,,,,,,,
1st_infer_latency: Text/Code generation models: Fisrt inference latency,,,,,,,,,,,,,,,,,,,,,,
2nd_infer_avg_latency: Text/Code generation models: Other inferences (exclude first inference) latency,,,,,,,,,,,,,,,,,,,,,,
result_md5: MD5 of generated text,,,,,,,,,,,,,,,,,,,,,,
prompt_idx: Index of prompts,,,,,,,,,,,,,,,,,,,,,,
tokenization_time: Tokenizer encode time,,,,,,,,,,,,,,,,,,,,,,
detokenization_time: Tokenizer decode time,,,,,,,,,,,,,,,,,,,,,,
pretrain_time: Total time of load model and compile model,,,,,,,,,,,,,,,,,,,,,,
generation_time: Time for one interaction. (e.g. The duration of  answering one question or generating one picture),,,,,,,,,,,,,,,,,,,,,,
iteration=0: warm-up; iteration=-1: average (exclude warm-up),,,,,,,,,,,,,,,,,,,,,,
max_rss_mem: max rss memory consumption;the value in -1 iteration row is the maximum value of all available RSS memory numbers in iterations,,,,,,,,,,,,,,,,,,,,,,
max_shared_mem: max shared memory consumption;the value in -1 iteration row is the maximum value of all available shared memory numbers in iterations,,,,,,,,,,,,,,,,,,,,,,
